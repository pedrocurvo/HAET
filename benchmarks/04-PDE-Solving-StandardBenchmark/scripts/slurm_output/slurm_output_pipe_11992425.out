============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
Running experiment on Pipe dataset
/home/scur1405/.local/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
wandb: Currently logged in as: salvatorpes (salvatorpes-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/sw/arch/RHEL9/EB_production/2024/software/Anaconda3/2024.06-1/lib/python3.12/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:
  Expected `list[str]` but got `tuple` - serialized value may not be as expected
  Expected `list[str]` but got `tuple` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
/sw/arch/RHEL9/EB_production/2024/software/Anaconda3/2024.06-1/lib/python3.12/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:
  Expected `list[str]` but got `tuple` - serialized value may not be as expected
  Expected `list[str]` but got `tuple` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/home1/scur1405/HAET/benchmarks/04-PDE-Solving-StandardBenchmark/wandb/run-20250523_142221-mik6nsk4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pipe_HAETransolver
wandb: ‚≠êÔ∏è View project at https://wandb.ai/salvatorpes-university-of-amsterdam/PDE-Solving
wandb: üöÄ View run at https://wandb.ai/salvatorpes-university-of-amsterdam/PDE-Solving/runs/mik6nsk4
/gpfs/home1/scur1405/HAET/benchmarks/04-PDE-Solving-StandardBenchmark/exp_pipe.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler() if args.use_amp else None
torch.Size([2310, 129, 129, 2]) torch.Size([2310, 129, 129])
Dataloading is over.
Namespace(lr=0.001, epochs=500, weight_decay=1e-05, model='HAETransolver_Structured_Mesh_2D', n_hidden=128, n_layers=8, n_heads=8, batch_size=8, gpu='0', max_grad_norm=0.1, downsamplex=1, downsampley=1, mlp_ratio=2, dropout=0.0, unified_pos=0, ref=8, slice_num=64, eval=1, save_name='pipe_HAETransolver', data_path='./data/pipe', use_wandb=1, wandb_project='PDE-Solving', wandb_entity=None, use_amp=1)
OptimizedModule(
  (_orig_mod): Model(
    (preprocess): MLP(
      (linear_pre): Sequential(
        (0): Linear(in_features=2, out_features=256, bias=True)
        (1): GELU(approximate='none')
      )
      (linear_post): Linear(in_features=256, out_features=128, bias=True)
      (linears): ModuleList()
    )
    (blocks): ModuleList(
      (0-6): 7 x TransolverErwinBlock(
        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (Attn): Physics_Attention_Structured_Mesh_2D(
          (in_project_x): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (in_project_slice): Linear(in_features=16, out_features=64, bias=True)
          (ada_temp_linear): Linear(in_features=16, out_features=1, bias=True)
          (erwin): ErwinTransformer(
            (encoder): ModuleList(
              (0): BasicLayer(
                (blocks): ModuleList(
                  (0-1): 2 x ErwinTransformerBlock(
                    (norm1): RMSNorm((16,), eps=None, elementwise_affine=True)
                    (norm2): RMSNorm((16,), eps=None, elementwise_affine=True)
                    (BMSA): BallMSA(
                      (qkv): Linear(in_features=16, out_features=48, bias=True)
                      (proj): Linear(in_features=16, out_features=16, bias=True)
                      (pe_proj): Linear(in_features=2, out_features=16, bias=True)
                    )
                    (swiglu): SwiGLU(
                      (w1): Linear(in_features=16, out_features=32, bias=True)
                      (w2): Linear(in_features=16, out_features=32, bias=True)
                      (w3): Linear(in_features=32, out_features=16, bias=True)
                    )
                  )
                )
                (pool): BallPooling(
                  (proj): Linear(in_features=36, out_features=32, bias=True)
                  (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
            )
            (bottleneck): BasicLayer(
              (blocks): ModuleList(
                (0-1): 2 x ErwinTransformerBlock(
                  (norm1): RMSNorm((32,), eps=None, elementwise_affine=True)
                  (norm2): RMSNorm((32,), eps=None, elementwise_affine=True)
                  (BMSA): BallMSA(
                    (qkv): Linear(in_features=32, out_features=96, bias=True)
                    (proj): Linear(in_features=32, out_features=32, bias=True)
                    (pe_proj): Linear(in_features=2, out_features=32, bias=True)
                  )
                  (swiglu): SwiGLU(
                    (w1): Linear(in_features=32, out_features=64, bias=True)
                    (w2): Linear(in_features=32, out_features=64, bias=True)
                    (w3): Linear(in_features=64, out_features=32, bias=True)
                  )
                )
              )
            )
            (decoder): ModuleList(
              (0): BasicLayer(
                (blocks): ModuleList(
                  (0-1): 2 x ErwinTransformerBlock(
                    (norm1): RMSNorm((16,), eps=None, elementwise_affine=True)
                    (norm2): RMSNorm((16,), eps=None, elementwise_affine=True)
                    (BMSA): BallMSA(
                      (qkv): Linear(in_features=16, out_features=48, bias=True)
                      (proj): Linear(in_features=16, out_features=16, bias=True)
                      (pe_proj): Linear(in_features=2, out_features=16, bias=True)
                    )
                    (swiglu): SwiGLU(
                      (w1): Linear(in_features=16, out_features=32, bias=True)
                      (w2): Linear(in_features=16, out_features=32, bias=True)
                      (w3): Linear(in_features=32, out_features=16, bias=True)
                    )
                  )
                )
                (unpool): BallUnpooling(
                  (proj): Linear(in_features=36, out_features=32, bias=True)
                  (norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
            )
          )
          (to_out): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (linear_pre): Sequential(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): GELU(approximate='none')
          )
          (linear_post): Linear(in_features=256, out_features=128, bias=True)
          (linears): ModuleList()
        )
      )
      (7): TransolverErwinBlock(
        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (Attn): Physics_Attention_Structured_Mesh_2D(
          (in_project_x): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (in_project_slice): Linear(in_features=16, out_features=64, bias=True)
          (ada_temp_linear): Linear(in_features=16, out_features=1, bias=True)
          (erwin): ErwinTransformer(
            (encoder): ModuleList(
              (0): BasicLayer(
                (blocks): ModuleList(
                  (0-1): 2 x ErwinTransformerBlock(
                    (norm1): RMSNorm((16,), eps=None, elementwise_affine=True)
                    (norm2): RMSNorm((16,), eps=None, elementwise_affine=True)
                    (BMSA): BallMSA(
                      (qkv): Linear(in_features=16, out_features=48, bias=True)
                      (proj): Linear(in_features=16, out_features=16, bias=True)
                      (pe_proj): Linear(in_features=2, out_features=16, bias=True)
                    )
                    (swiglu): SwiGLU(
                      (w1): Linear(in_features=16, out_features=32, bias=True)
                      (w2): Linear(in_features=16, out_features=32, bias=True)
                      (w3): Linear(in_features=32, out_features=16, bias=True)
                    )
                  )
                )
                (pool): BallPooling(
                  (proj): Linear(in_features=36, out_features=32, bias=True)
                  (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
            )
            (bottleneck): BasicLayer(
              (blocks): ModuleList(
                (0-1): 2 x ErwinTransformerBlock(
                  (norm1): RMSNorm((32,), eps=None, elementwise_affine=True)
                  (norm2): RMSNorm((32,), eps=None, elementwise_affine=True)
                  (BMSA): BallMSA(
                    (qkv): Linear(in_features=32, out_features=96, bias=True)
                    (proj): Linear(in_features=32, out_features=32, bias=True)
                    (pe_proj): Linear(in_features=2, out_features=32, bias=True)
                  )
                  (swiglu): SwiGLU(
                    (w1): Linear(in_features=32, out_features=64, bias=True)
                    (w2): Linear(in_features=32, out_features=64, bias=True)
                    (w3): Linear(in_features=64, out_features=32, bias=True)
                  )
                )
              )
            )
            (decoder): ModuleList(
              (0): BasicLayer(
                (blocks): ModuleList(
                  (0-1): 2 x ErwinTransformerBlock(
                    (norm1): RMSNorm((16,), eps=None, elementwise_affine=True)
                    (norm2): RMSNorm((16,), eps=None, elementwise_affine=True)
                    (BMSA): BallMSA(
                      (qkv): Linear(in_features=16, out_features=48, bias=True)
                      (proj): Linear(in_features=16, out_features=16, bias=True)
                      (pe_proj): Linear(in_features=2, out_features=16, bias=True)
                    )
                    (swiglu): SwiGLU(
                      (w1): Linear(in_features=16, out_features=32, bias=True)
                      (w2): Linear(in_features=16, out_features=32, bias=True)
                      (w3): Linear(in_features=32, out_features=16, bias=True)
                    )
                  )
                )
                (unpool): BallUnpooling(
                  (proj): Linear(in_features=36, out_features=32, bias=True)
                  (norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
            )
          )
          (to_out): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (linear_pre): Sequential(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): GELU(approximate='none')
          )
          (linear_post): Linear(in_features=256, out_features=128, bias=True)
          (linears): ModuleList()
        )
        (ln_3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp2): Linear(in_features=128, out_features=1, bias=True)
      )
    )
  )
/gpfs/home1/scur1405/HAET/benchmarks/04-PDE-Solving-StandardBenchmark/exp_pipe.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("./checkpoints/" + save_name + ".pt"), strict=False)
/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
AUTOTUNE mm(133128x2, 2x256)
  mm 0.1024 ms 100.0% 
  triton_mm_8 0.1167 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_10 0.1167 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_11 0.1167 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_14 0.1229 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_13 0.1290 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_15 0.1290 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12 0.1731 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_5 0.1741 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_7 0.1741 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5191 seconds and 15.8458 seconds precompiling
E0523 14:23:20.713000 3966779 /gpfs/home1/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:1300] [1/0] Exception out of resource: shared memory, Required: 262144, Hardware limit: 166912. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/scratch-local/scur1405.11992425/torchinductor_scur1405/gw/cgwydsd2iqamnenxft4y7dg3igmegpfvs5qvkwfbtmq2m7h2oyu2.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8)
E0523 14:23:20.715000 3966779 /gpfs/home1/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:1300] [1/0] Exception out of resource: shared memory, Required: 262144, Hardware limit: 166912. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/scratch-local/scur1405.11992425/torchinductor_scur1405/yv/cyvinz52vaxowwjw53ctraq6xq3vv5t3hnkc2cole4yx7ywax5h5.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4)
E0523 14:23:20.716000 3966779 /gpfs/home1/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:1300] [1/0] Exception out of resource: shared memory, Required: 294912, Hardware limit: 166912. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/scratch-local/scur1405.11992425/torchinductor_scur1405/kt/cktrr27fgnidyppgmuv7bijrngumspd7jswa3ozshfmo5rjns64u.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4)
E0523 14:23:20.717000 3966779 /gpfs/home1/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:1300] [1/0] Exception out of resource: shared memory, Required: 196608, Hardware limit: 166912. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/scratch-local/scur1405.11992425/torchinductor_scur1405/vq/cvqc4kabcp64abg3rh2rlftdag62eny5wkf7mbdf2ejyenvy7g6h.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4)
W0523 14:23:21.711000 3966779 /gpfs/home1/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:1517] [1/0] out of resource: shared memory, Required: 196608, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.
W0523 14:23:22.109000 3966779 /gpfs/home1/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:1517] [1/0] out of resource: shared memory, Required: 262144, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.
W0523 14:23:22.498000 3966779 /gpfs/home1/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:1517] [1/0] out of resource: shared memory, Required: 294912, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.
W0523 14:23:23.146000 3966779 /gpfs/home1/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:1517] [1/0] out of resource: shared memory, Required: 262144, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.
AUTOTUNE addmm(133128x128, 133128x256, 256x128)
  addmm 0.5693 ms 100.0% 
  triton_mm_32 0.6205 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  bias_addmm 0.6226 ms 91.4% 
  triton_mm_29 0.7608 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_25 0.7629 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_31 0.8335 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_27 0.9032 ms 63.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_22 1.0844 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_26 1.1162 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_30 1.1172 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 2.4292 seconds and 0.0059 seconds precompiling
AUTOTUNE convolution(8x128x129x129, 128x128x3x3)
  triton_convolution2d_40 0.4915 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_35 0.4966 ms 99.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_36 0.5601 ms 87.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_38 0.5622 ms 87.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_39 0.5939 ms 82.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_41 0.7014 ms 70.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_37 4.1492 ms 11.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8
  convolution 760.0865 ms 0.1% 
SingleProcess AUTOTUNE benchmarking takes 4.7457 seconds and 0.0007 seconds precompiling
AUTOTUNE mm(1065024x16, 16x64)
  triton_mm_53 0.2796 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  mm 0.2908 ms 96.1% 
  triton_mm_47 0.3359 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_49 0.3359 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_50 0.3359 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_52 0.3359 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_54 0.3471 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_55 0.3471 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_56 0.3471 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_48 0.5622 ms 49.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.9523 seconds and 0.0017 seconds precompiling
AUTOTUNE mm(1065024x16, 16x1)
  mm 0.0963 ms 100.0% 
  triton_mm_64 0.1270 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_60 0.1782 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_61 0.1782 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_62 0.1782 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_63 0.1782 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_65 0.1792 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_66 0.1792 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_67 0.1792 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_57 0.1823 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 1.3924 seconds and 0.0006 seconds precompiling
AUTOTUNE bmm(64x16x16644, 64x16644x64)
  triton_bmm_76 0.3389 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_79 0.3389 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_80 0.3451 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_75 0.3512 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_70 0.3738 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_78 0.3758 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_77 0.4014 ms 84.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  bmm 0.4925 ms 68.8% 
  triton_bmm_71 0.5110 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=2
  triton_bmm_69 0.5806 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 1.7190 seconds and 0.0012 seconds precompiling
)
Total Trainable Params: 2167305
Traceback (most recent call last):
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1446, in _call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 129, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/__init__.py", line 2235, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1253, in compile_fx
    return compile_fx(
           ^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1521, in compile_fx
    return aot_autograd(
           ^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/backends/common.py", line 72, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1071, in aot_module_simplified
    compiled_fn = dispatch_and_compile()
                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1056, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 522, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 759, in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
                               ^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 179, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1350, in fw_compiler_base
    return _fw_compiler_base(model, example_inputs, is_inference)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1359, in _fw_compiler_base
    _recursive_joint_graph_passes(model)
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 281, in _recursive_joint_graph_passes
    joint_graph_passes(gm)
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/fx_passes/joint_graph.py", line 460, in joint_graph_passes
    count += patterns.apply(graph.graph)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/pattern_matcher.py", line 1729, in apply
    if is_match(m) and entry.extra_check(m):
                       ^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/pattern_matcher.py", line 1331, in check_fn
    if is_match(specific_pattern_match) and extra_check(specific_pattern_match):
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/fx_passes/pad_mm.py", line 146, in should_pad_addmm
    return should_pad_common(mat1, mat2, input) and should_pad_bench(
                                                    ^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/fx_passes/pad_mm.py", line 566, in should_pad_bench
    ori_time = do_bench(orig_bench_fn)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/runtime/benchmarking.py", line 66, in wrapper
    return fn(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/runtime/benchmarking.py", line 201, in benchmark_gpu
    return self.triton_do_bench(_callable, **kwargs, return_mode="median")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/triton/testing.py", line 106, in do_bench
    fn()
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_inductor/fx_passes/pad_mm.py", line 537, in orig_bench_fn
    op(input, mat1, mat2)
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_ops.py", line 1116, in __call__
    return self._op(*args, **(kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/gpfs/home1/scur1405/HAET/benchmarks/04-PDE-Solving-StandardBenchmark/exp_pipe.py", line 298, in <module>
    main()
  File "/gpfs/home1/scur1405/HAET/benchmarks/04-PDE-Solving-StandardBenchmark/exp_pipe.py", line 166, in main
    out = model(x, None).squeeze(-1)
          ^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 465, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home1/scur1405/HAET/models/HAETransolver_Structured_Mesh_2D.py", line 412, in forward
    fx = block(fx)
         ^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home1/scur1405/HAET/models/HAETransolver_Structured_Mesh_2D.py", line 123, in forward
    def forward(self, fx):
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home1/scur1405/HAET/models/PhysicsAttention/StructuredMesh2D.py", line 157, in forward
    def forward(self, x):
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home1/scur1405/HAET/models/components/erwinflash/erwin_flash.py", line 138, in forward
    def forward(
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1269, in __call__
    return self._torchdynamo_orig_callable(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1064, in __call__
    result = self._inner_convert(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 526, in __call__
    return _compile(
           ^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 924, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 666, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_utils_internal.py", line 87, in wrapper_function
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 699, in _compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1322, in transform_code_object
    transformations(instructions, code_options)
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 219, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 634, in transform
    tracer.run()
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 2796, in run
    super().run()
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
    while self.step():
          ^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 580, in wrapper
    return handle_graph_break(self, inst, speculation.reason)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 649, in handle_graph_break
    self.output.compile_subgraph(self, reason=reason)
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1142, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1369, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1416, in call_user_compiler
    return self._call_user_compiler(gm)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1405/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1465, in _call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mpipe_HAETransolver[0m at: [34mhttps://wandb.ai/salvatorpes-university-of-amsterdam/PDE-Solving/runs/mik6nsk4[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../gpfs/home1/scur1405/HAET/benchmarks/04-PDE-Solving-StandardBenchmark/wandb/run-20250523_142221-mik6nsk4/logs[0m
srun: error: gcn15: task 0: Exited with exit code 1
srun: Terminating StepId=11992425.0
Experiment completed. Check the output files for results.

JOB STATISTICS
==============
Job ID: 11992425
Cluster: snellius
User/Group: scur1405/scur1405
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:01:43
CPU Efficiency: 4.12% of 00:41:42 core-walltime
Job Wall-clock time: 00:02:19
Memory Utilized: 2.35 GB
Memory Efficiency: 1.95% of 120.00 GB (120.00 GB/node)
